{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación de textos\n",
    "\n",
    "En esta clase, exploraremos datos de mensajes de texto y crearemos modelos para predecir si un mensaje es spam o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Go until jurong point, crazy.. Available only ...       0\n",
       "1                      Ok lar... Joking wif u oni...       0\n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...       1\n",
       "3  U dun say so early hor... U c already then say...       0\n",
       "4  Nah I don't think he goes to usf, he lives aro...       0\n",
       "5  FreeMsg Hey there darling it's been 3 week's n...       1\n",
       "6  Even my brother is not like to speak with me. ...       0\n",
       "7  As per your request 'Melle Melle (Oru Minnamin...       0\n",
       "8  WINNER!! As a valued network customer you have...       1\n",
       "9  Had your mobile 11 months or more? U R entitle...       1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "spam_data = pd.read_csv('spam.csv')\n",
    "\n",
    "spam_data['target'] = np.where(spam_data['target']=='spam',1,0)\n",
    "spam_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>target_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "      <td>no_spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "      <td>no_spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "      <td>no_spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "      <td>no_spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>1</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>0</td>\n",
       "      <td>no_spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "      <td>0</td>\n",
       "      <td>no_spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "      <td>1</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "      <td>1</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target target_text\n",
       "0  Go until jurong point, crazy.. Available only ...       0     no_spam\n",
       "1                      Ok lar... Joking wif u oni...       0     no_spam\n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...       1        spam\n",
       "3  U dun say so early hor... U c already then say...       0     no_spam\n",
       "4  Nah I don't think he goes to usf, he lives aro...       0     no_spam\n",
       "5  FreeMsg Hey there darling it's been 3 week's n...       1        spam\n",
       "6  Even my brother is not like to speak with me. ...       0     no_spam\n",
       "7  As per your request 'Melle Melle (Oru Minnamin...       0     no_spam\n",
       "8  WINNER!! As a valued network customer you have...       1        spam\n",
       "9  Had your mobile 11 months or more? U R entitle...       1        spam"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_data['target_text']= spam_data['target'].apply(lambda x: 'spam' if x else 'no_spam')\n",
    "spam_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(spam_data['text'], \n",
    "                                                    spam_data['target'], \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4179,) (4179,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 1\n",
    "¿Qué porcentaje de los documentos en `spam_data` son spam?\n",
    "\n",
    "*Esta función debe devolver un valor flotante, el valor porcentual (es decir, $ ratio * 100 $).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respuesta_uno():\n",
    "    spam_data['target'].mean()    #Calcula el promedio de mesajes spam, pero no lo usa(?)\n",
    "    spam=spam_data[spam_data['target'] != 0]    #Obtiene los elementos que son spam\n",
    "    print(len(spam), len(spam_data))    #Imprime la relación entre mensaje totales y spam\n",
    "    return spam_data['target'].mean()*100   #Regresa el promedio de datos (porcentaje de documentos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(747, 5572)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13.406317300789663"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta_uno()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 2\n",
    "\n",
    "Ajustar y transformar los datos de entrenamiento `X_train` utilizando un `count_vectorizer` con parámetros predeterminados.\n",
    "\n",
    "Luego, ajuste un modelo de clasificación Naive Bayes multinomial. Calcule medidas de exactitud, presición, recall y f1-score usando los datos de prueba transformados.\n",
    "\n",
    "*Esta función debe devolver las cuatro medidas de evaluación como una lista con los 4 valores en el orden solicitado cada valor como flotante.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def respuesta_dos():\n",
    "    scores=[]\n",
    "    \n",
    "    vect = CountVectorizer().fit(X_train) #Learn a vocabulary dictionary of all tokens in the raw documents.\n",
    "    X_train_vectorized=vect.transform(X_train)  #Transform documents to document-term matrix.\n",
    "    \n",
    "    clf= MultinomialNB()  #Naive Bayes classifier for multinomial models.\n",
    "    clf.fit( X_train_vectorized, y_train ) #Fit Naive Bayes classifier according to X, y.\n",
    "    print(\"matriz Frecuencias shape:\",X_train_vectorized.shape, \"num clases\",len(y_train))\n",
    "    \n",
    "    X_test_vectorized= vect.transform( X_test )\n",
    "    predictions = clf.predict(X_test_vectorized)\n",
    "    print(\"matriz Frecuencias Test shape:\",X_test_vectorized.shape, \"num prediction\",len(predictions))\n",
    "    \n",
    "    #Medidas de evaluación\n",
    "    scores.append(accuracy_score(y_test,predictions))\n",
    "    scores.append(precision_score(y_test,predictions))\n",
    "    scores.append(recall_score(y_test,predictions))\n",
    "    scores.append(f1_score(y_test,predictions))\n",
    "    \n",
    "    #Imprime los valores de la predicción de los textos de spam\n",
    "    spam=predictions[predictions!= 0]\n",
    "    print('num Predicciones Spam:',len(spam))\n",
    "    print('Porcentaje Spam:',(len(spam)/len(predictions))*100)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n",
    "    print(confusion_matrix(y_test, predictions))\n",
    "    print(tn, fp, fn, tp)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('matriz Frecuencias:', (4179, 7354), 'clases', 4179)\n",
      "('matriz Frecuencias Test:', (1393, 7354), 'prediction', 1393)\n",
      "('Predicciones Spam:', 184)\n",
      "('Porcentaje Spam:', 0)\n",
      "[[1193    3]\n",
      " [  16  181]]\n",
      "(1193, 3, 16, 181)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9863603732950467,\n",
       " 0.9836956521739131,\n",
       " 0.9187817258883249,\n",
       " 0.9501312335958005]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta_dos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 3\n",
    "\n",
    "Ajustar y transformar los datos de entrenamiento `X_train` utilizando un `count_vectorizer` con parámetros predeterminados.\n",
    "\n",
    "Luego, ajuste un modelo de clasificación Naive Bayes multinomial con suavizado (smoothing) `alpha = 0.1`. Encuentre el área bajo la curva (AUC) usando los datos de prueba transformados.\n",
    "\n",
    "*Esta función debe devolver el AUC como un flotante.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def respuesta_tres():\n",
    "    vect = CountVectorizer().fit(X_train)\n",
    "    X_train_vectorized=vect.transform(X_train)\n",
    "    clf=MultinomialNB(alpha=0.1)\n",
    "    clf.fit(X_train_vectorized, y_train)\n",
    "    X_test_vectorized=vect.transform(X_test)\n",
    "    predictions = clf.predict(X_test_vectorized)\n",
    "    print(predictions)\n",
    "    return roc_auc_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9720812182741116"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta_tres()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 4\n",
    "\n",
    "Ajustar y transformar los datos de entrenamiento `X_train` utilizando un `TfidfVectorizer` ignorando los términos que tienen una frecuencia de documento estrictamente inferior a **3**.\n",
    "\n",
    "Luego, ajuste un modelo de clasificador Naive Bayes multinomial con suavizado (smoothing) `alfa = 0.1` y calcule el área bajo de la curva (AUC) usando los datos de prueba transformados.\n",
    "\n",
    "*Esta función debe devolver el AUC como un flotante.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def respuesta_cuatro():\n",
    "    #Genero un objeto vectorizer y Learn vocabulary and idf from training set.\n",
    "    vectorizer= TfidfVectorizer( min_df=3 ).fit( X_train )\n",
    "    #Transform documents to document-term matrix.\n",
    "    X_train_vectorizer= vectorizer.transform( X_train )\n",
    "    \n",
    "    #Generación de modelo Naive-Bayes con suavizado y ajuste con los datos generados arriba\n",
    "    clf= MultinomialNB(alpha=0.1)\n",
    "    clf.fit( X_train_vectorizer, y_train )\n",
    "    \n",
    "    #Predicciones, obtenemos la matriz de terminos y se lo pasamos al modelo\n",
    "    X_test_vectorizerd= vectorizer.transform( X_test )\n",
    "    predictions= clf.predict( X_test_vectorizerd )\n",
    "    \n",
    "    \n",
    "    print(predictions)\n",
    "    return roc_auc_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9416243654822335"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta_cuatro()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 5\n",
    "\n",
    "¿Cuál es la longitud promedio de los documentos (número de caracteres) para los documentos spam y no spam?\n",
    "\n",
    "*Esta función debe devolver una tupla (longitud promedio no spam, longitud promedio de spam).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respuesta_cinco():\n",
    "    return spam_data[ spam_data['target'] == 1 ]['text'].str.count('\\w').mean(), spam_data[ spam_data['target'] == 0 ]['text'].str.count('\\w').mean()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109.82463186077644, 53.73181347150259)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta_cinco()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "The following function has been provided to help you combine new features into the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature(X, feature_to_add):\n",
    "    \"\"\"\n",
    "    Returns sparse feature matrix with added feature.\n",
    "    feature_to_add can also be a list of features.\n",
    "    \"\"\"\n",
    "    from scipy.sparse import csr_matrix, hstack\n",
    "    return hstack([X, csr_matrix(feature_to_add).T], 'csr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 6\n",
    "\n",
    "Ajustar y transformar los datos de entrenamiento `X_train` usando un `TfidfVectorizer` ignorando los términos que tienen una frecuencia de documento estrictamente inferior a **5**.\n",
    "\n",
    "Usando esta matriz de término de documento y una característica adicional, **la longitud del documento (número de caracteres)**, ajustar a un modelo de Clasificación de Vector de Soporte con regularización `C = 10000`. Luego calcule el área bajo de la curva (AUC) usando los datos de prueba transformados.\n",
    "\n",
    "*Esta función debe devolver el AUC como un flotante.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "def respuesta_seis():\n",
    "    # to count the number of words\n",
    "    #cv= CountVectorizer() \n",
    "    #word_count_vector= cv.fit_transform( X_train )\n",
    "    #count_vector= cv.transform( X_train )\n",
    "    #vectorizer= TfidfVectorizer( min_df=5 ).fit( word_count_vector )\n",
    "    #X_train_vectorizer= vectorizer.transform( count_vector )\n",
    "    \n",
    "    #Genero un objeto vectorizer y Learn vocabulary and idf from training set.\n",
    "    vectorizer= TfidfVectorizer( min_df=3 ).fit( X_train )\n",
    "    #Transform documents to document-term matrix.\n",
    "    X_train_vectorizer= vectorizer.transform( X_train )\n",
    "    #Añadimos una nueva caracteristica\n",
    "    X_train_vectorizer= add_feature( X_train_vectorizer, X_train.str.count('\\w') )\n",
    "    \n",
    "    #Creamos el modelo y entrenamos\n",
    "    clf= SVC( C= 10000 )\n",
    "    #clf = make_pipeline(StandardScaler(), SVC(C=10000))\n",
    "    clf.fit( X_train_vectorizer, y_train )\n",
    "\n",
    "    #Predicciones, obtenemos la matriz de terminos y se lo pasamos al modelo\n",
    "    X_test_vectorizerd= vectorizer.transform( X_test )\n",
    "    X_test_vectorizerd= add_feature( X_test_vectorizerd, X_test.str.count('\\w') )\n",
    "    \n",
    "    \n",
    "    predictions= clf.predict( X_test_vectorizerd )\n",
    "    \n",
    "    \n",
    "    print(predictions)\n",
    "    return roc_auc_score(y_test,predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9657508955401253"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta_seis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 7\n",
    "\n",
    "¿Cuál es el número promedio de dígitos por documento para los documentos no spam y spam?\n",
    "\n",
    "*Esta función debe devolver una tupla (promedio de # dígitos no es spam, promedio # dígitos spam).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respuesta_siete():\n",
    "    return spam_data[ spam_data['target'] == 1 ]['text'].str.count('\\d').mean(), spam_data[ spam_data['target'] == 0 ]['text'].str.count('\\d').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15.759036144578314, 0.2992746113989637)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta_siete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 8\n",
    "\n",
    "Ajustar y transformar los datos de entrenamiento `X_train` usando un `TfidfVectorizer` ignorando los términos que tienen una frecuencia de documento estrictamente inferior a **5** y usando **n-gramas de palabras n = 1 a n = 3** (unigramas, bigramas y trigramas).\n",
    "\n",
    "Usando esta matriz de término-documento y las siguientes características adicionales:\n",
    "* la longitud del documento (número de caracteres)\n",
    "* **cantidad de dígitos por documento**\n",
    "\n",
    "Ajustar un modelo de Regresión logística con regularización `C = 100`. Luego calcule el área bajo de la curva (AUC) usando los datos de prueba transformados.\n",
    "\n",
    "*Esta función debe devolver el AUC como un flotante.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def respuesta_ocho():\n",
    "    #Genero un objeto vectorizer y Learn vocabulary and idf from training set.\n",
    "    vectorizer= TfidfVectorizer( ngram_range=(1,3), min_df=5 ).fit( X_train )\n",
    "    #Transform documents to document-term matrix.\n",
    "    X_train_vectorizer= vectorizer.transform( X_train )\n",
    "    #Añadimos nuevas caracteristica\n",
    "    X_train_vectorizer= add_feature( X_train_vectorizer, X_train.str.count('\\w') )\n",
    "    X_train_vectorizer= add_feature( X_train_vectorizer, X_train.str.count('\\d') )\n",
    "    \n",
    "    #Creamos el modelo y entrenamos\n",
    "    clf= LogisticRegression( C= 100 )\n",
    "    clf.fit( X_train_vectorizer, y_train )\n",
    "\n",
    "    #Predicciones, obtenemos la matriz de terminos y se lo pasamos al modelo\n",
    "    X_test_vectorizerd= vectorizer.transform( X_test )\n",
    "    X_test_vectorizerd= add_feature( X_test_vectorizerd, X_test.str.count('\\w') )\n",
    "    X_test_vectorizerd= add_feature( X_test_vectorizerd, X_test.str.count('\\d') )\n",
    "    predictions= clf.predict( X_test_vectorizerd )\n",
    "    \n",
    "    \n",
    "    print(predictions)\n",
    "    return roc_auc_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aceron/opt/miniconda3/envs/maestria/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9708270376721049"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta_ocho()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 9\n",
    "\n",
    "¿Cuál es el número promedio de caracteres que no son palabras (cualquier cosa que no sea una letra, un dígito o un guión bajo) por documento para los documentos que no son spam y spam?\n",
    "\n",
    "*Sugerencia: utilice las clases de caracteres `\\ w` y` \\ W`*\n",
    "\n",
    "*Esta función debe devolver una tupla (promedio de # caracteres que no son palabras, no spam, promedio de # caracteres que no son palabras, spam).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pregunta_nueve():\n",
    "        return spam_data[ spam_data['target'] == 1 ]['text'].str.count('\\W').mean(), spam_data[ spam_data['target'] == 0 ]['text'].str.count('\\W').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29.041499330655956, 17.29181347150259)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pregunta_nueve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 10\n",
    "\n",
    "Ajustar y transformar los datos de entrenamiento `X_train` usando un `CountVectorizer` ignorando los términos que tienen una frecuencia de documento estrictamente inferior a **5** y utilizando **caracteres n-grams desde n = 2 a n = 5.**\n",
    "\n",
    "Para decirle a `CountVectorizer` que use caracteres n-grams, pase en `analyzer = 'char_wb'` que crea caracteres n-gramas solo del texto dentro de los límites de las palabras. Esto debería hacer que el modelo sea más robusto a los errores ortográficos.\n",
    "\n",
    "Usando esta matriz término documento y las siguientes características adicionales:\n",
    "* la longitud del documento (número de caracteres)\n",
    "* cantidad de dígitos por documento\n",
    "* **cantidad de caracteres que no son palabras (cualquier cosa que no sea una letra, dígito o guión bajo).**\n",
    "\n",
    "Ajustar un modelo de Regresión logística con regularización `C = 100`. Luego calcule el área bajo de la curva (AUC) usando los datos de prueba transformados.\n",
    "\n",
    "También **encuentre los 10 coeficientes más pequeños y los 10 más grandes del modelo** y devuélvalos junto con el AUC en una tupla.\n",
    "\n",
    "La lista de los 10 coeficientes más pequeños debe ordenarse primero, la lista de los 10 coeficientes más grandes debe ordenarse primero.\n",
    "\n",
    "Las tres características que se agregaron a la matriz de términos del documento deben tener los siguientes nombres si aparecen en la lista de coeficientes:\n",
    "['longitud_doc', 'conteo_digito', 'caracteres_no_palabra']\n",
    "\n",
    "*Esta función debe devolver una tupla `(AUC como flotante, lista de coeficientes más pequeños, lista de coeficientes más grande)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def respuesta_10():\n",
    "    vect = CountVectorizer( analyzer = 'char_wb', ngram_range=(2,5), max_df=5 ).fit(X_train) #Learn a vocabulary dictionary of all tokens in the raw documents.\n",
    "    X_train_vectorizer=vect.transform(X_train)  #Transform documents to document-term matrix.\n",
    "    #Añadimos nuevas caracteristica\n",
    "    X_train_vectorizer= add_feature( X_train_vectorizer, X_train.str.count('\\W') )\n",
    "    X_train_vectorizer= add_feature( X_train_vectorizer, X_train.str.count('\\w') )\n",
    "    X_train_vectorizer= add_feature( X_train_vectorizer, X_train.str.count('\\d') )\n",
    "    \n",
    "    clf= LogisticRegression( C=100 )  #Naive Bayes classifier for multinomial models.\n",
    "    clf.fit( X_train_vectorizer, y_train ) #Fit Naive Bayes classifier according to X, y.\n",
    "    \n",
    "    X_test_vectorizerd= vect.transform( X_test )\n",
    "    X_test_vectorizerd= add_feature( X_test_vectorizerd, X_test.str.count('\\W') )\n",
    "    X_test_vectorizerd= add_feature( X_test_vectorizerd, X_test.str.count('\\w') )\n",
    "    X_test_vectorizerd= add_feature( X_test_vectorizerd, X_test.str.count('\\d') )\n",
    "    predictions = clf.predict(X_test_vectorizerd)\n",
    "    \n",
    "    coef= clf.coef_\n",
    "    lowr= sorted(coef)[:10]\n",
    "    uppr= sorted(coef,reverse=True)\n",
    "    \n",
    "    scores=[]\n",
    "    #Medidas de evaluación\n",
    "    scores.append(accuracy_score(y_test,predictions))\n",
    "    scores.append(precision_score(y_test,predictions))\n",
    "    scores.append(recall_score(y_test,predictions))\n",
    "    scores.append(f1_score(y_test,predictions))\n",
    "    \n",
    "    #Imprime los valores de la predicción de los textos de spam\n",
    "    spam=predictions[predictions!= 0]\n",
    "    print('num Predicciones Spam:',len(spam))\n",
    "    print('Porcentaje Spam:',(len(spam)/len(predictions))*100)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n",
    "    print(confusion_matrix(y_test, predictions))\n",
    "    print(tn, fp, fn, tp)\n",
    "    \n",
    "    \n",
    "    return scores, (lowr, uppr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num Predicciones Spam: 186\n",
      "Porcentaje Spam: 13.352476669059584\n",
      "[[1191    5]\n",
      " [  16  181]]\n",
      "1191 5 16 181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aceron/opt/miniconda3/envs/maestria/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.9849246231155779,\n",
       "  0.9731182795698925,\n",
       "  0.9187817258883249,\n",
       "  0.9451697127937337],\n",
       " ([array([-0.0177898 , -0.01550143, -0.00228836, ..., -0.42850894,\n",
       "           0.14614202,  1.64518111])],\n",
       "  [array([-0.0177898 , -0.01550143, -0.00228836, ..., -0.42850894,\n",
       "           0.14614202,  1.64518111])]))"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta_10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "python-text-mining",
   "graded_item_id": "Pn19K",
   "launcher_item_id": "y1juS",
   "part_id": "ctlgo"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('maestria')",
   "language": "python",
   "name": "python3100jvsc74a57bd07762ebd60544f968f8e269abea15df76463f6800ef56f050d75cc52a34f97881"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
